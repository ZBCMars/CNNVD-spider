# coding : UTF-8
import requests
import re
from bs4 import BeautifulSoup
import  urllib.request
from lxml import etree
import csv
from multiprocessing import Pool

fp = open('D:/finaldata.csv', 'a+', newline='')
writer = csv.writer(fp, dialect=("excel"))
writer.writerow(["chname", "cnvd_id", "bugtraq_id", "release_time", "vul_level", "impact_product", "cve_id",
                     "vul_description1", "vul_description2", "reference_link", "vul_solution", "vendor_patch",
                     "validation_info",
                     "submission_time", "included_time", "update_time", "vul_attachment", "Threat_type"])

def url_protocol(url):
    print('该站使用的协议是：' + re.findall(r'.*(?=://)',url)[0])
    return re.findall(r'.*(?=://)',url)[0]

def same_url(url):
    url = url.replace(url_protocol(url) + '://','')

    if re.findall(r'^www',url) == []:
        sameurl = 'www.' + url
        if sameurl.find('/') != -1:
            sameurl = re.findall(r'(?<=www.).*?(?=/)', sameurl)[0]
        else:
            sameurl = sameurl + '/'
            sameurl = re.findall(r'(?<=www.).*?(?=/)', sameurl)[0]
    else:
        if url.find('/') != -1:
            sameurl = re.findall(r'(?<=www.).*?(?=/)', url)[0]
        else:
            sameurl = url + '/'
            sameurl = re.findall(r'(?<=www.).*?(?=/)', sameurl)[0]
    print('同站域名地址：' + sameurl)
    return sameurl

class linkQuence:
    def __init__(self):
        self.visited = []
        self.unvisited = []

    def getVisitedUrl(self):
        return self.visited
    def getUnvisitedUrl(self):
        return self.unvisited
    def addVisitedUrl(self,url):
        return self.visited.append(url)
    def addUnvisitedUrl(self,url):
        if url != '' and url not in self.visited and url not in self.unvisited:
            return self.unvisited.insert(0,url)

    def removeVisited(self,url):
        return self.visited.remove(url)
    def popUnvisitedUrl(self):
        try:
            return self.unvisited.pop()
        except:
            return None
    def unvisitedUrlEmpty(self):
        return len(self.unvisited) == 0

class Spider():
    def __init__(self,url):
        self.linkQuence = linkQuence()
        self.linkQuence.addUnvisitedUrl(url)
        self.current_deepth = 1
    def getPageLinks(self,url):
        pageSource = requests.get(url).text
        pageLinks = re.findall(r'(?<=href=\").*?(?=title2\")|(?<=href=\').*?(?=title2\")',pageSource)
        return pageLinks

    def processUrl(self,url):
        true_url = []
        for l in self.getPageLinks(url):
            if re.findall(r'/',l):
                if re.findall(r':',l):
                    true_url.append(l)
                else:
                    true_url.append(url_protocol(url) + '://' + same_url(url) + l)
        for l in true_url:
            print(url + '该url页面源码中，有效url：' + l)
            aim = re.sub('" class="a_', "", l)
            '''page = urllib.urlopen(aim)
            soup = BeautifulSoup(page, 'html.parser')
            chname = soup.find()'''
            page = urllib.request.urlopen(aim)
            soup = BeautifulSoup(page, 'lxml')
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'
            }
            html = requests.get(aim, headers=headers)
            info = etree.HTML(html.text)
            chname = info.xpath('/html/body/div[4]/div/div[1]/div[2]/h2/text()')
            chname = re.sub("[\s+\!\/\[\]_,$%^*(+\"\')]+", "", str(chname))
            cnvd_id = info.xpath('/html/body/div[4]/div/div[1]/div[2]/ul/li[1]/span/text()')
            cnvd_id = re.sub("[\s+\!\/\[\]_,$%^*(+\"\')]+", "", str(cnvd_id))
            bugtraq_id = "none"
            release_time = info.xpath('/html/body/div[4]/div/div[1]/div[2]/ul/li[5]/a/text()')
            release_time = re.sub("[\s+\!\/\[\]_,$%^*(+\"\')]+", "", str(release_time))
            release_time = release_time.replace('\\r', '').replace('\\n', '').replace('\\t', '')
            pageSource = requests.get(aim).text
            vul_level = re.findall(r'(?<=\'vulnerability/querylist.tag).*?(?=\'\)\")', pageSource)
            vul_level = str(vul_level)
            vul_level = re.sub("[A-Za-z0-9\!\%\[\]\,\。]", "", vul_level)
            vul_level = re.sub("[\s+\!\/\[\]_,$%^*(+\"\')]+", "", str(vul_level))
            impact_producte = "none"
            cve_id = info.xpath('/html/body/div[4]/div/div[1]/div[2]/ul/li[3]/a/text()')
            cve_id = re.sub("[\s+\!\/\[\]_,$%^*(+\"\')]+", "", str(cve_id))
            cve_id = cve_id.replace('\\r', '').replace('\\n', '').replace('\\t', '')
            vul_description1 = info.xpath('/html/body/div[4]/div/div[1]/div[3]/p[1]/text()')
            vul_description1 = re.sub("[\s+\!\/\[\]_,$%^*(+\"\')]+", "", str(vul_description1))
            vul_description1 = vul_description1.replace('\\r', '').replace('\\n', '').replace('\\t', '')
            vul_description2 = info.xpath('/html/body/div[4]/div/div[1]/div[3]/p[2]/text()')
            vul_description2 = re.sub("[\s+\!\/\[\]_,$%^*(+\"\')]+", "", str(vul_description2))
            vul_description2 = vul_description2.replace('\\r', '').replace('\\n', '').replace('\\t', '')
            reference_link = info.xpath('/html/body/div[4]/div/div[1]/div[4]/p[2]/text()')
            reference_link = re.sub("[\s+\!\[\]_,$%^*(+\"\')]+", "", str(reference_link))
            reference_link = reference_link.replace('\\r', '').replace('\\n', '').replace('\\t', '')
            vul_solution = "none"
            vendor_patch = info.xpath('/html/body/div[4]/div/div[1]/div[4]/p[2]/text()')
            vendor_patch = re.sub("[\s+\!\[\]_,$%^*(+\"\')]+", "", str(vendor_patch))
            vendor_patch = vendor_patch.replace('\\r', '').replace('\\n', '').replace('\\t', '')
            validation_info = "none"
            submission_time = "none"
            included_time = "none"
            update_time = info.xpath('/html/body/div[4]/div/div[1]/div[2]/ul/li[7]/a/text()')
            update_time = re.sub("[\s+\!\/\[\]\\_,$%^*(+\"\')]+", "", str(update_time))
            update_time = update_time.replace('\\r', '').replace('\\n', '').replace('\\t', '')
            vul_attachment = "none"
            Threat_type = info.xpath('/html/body/div[4]/div/div[1]/div[2]/ul/li[6]/a/text()')
            Threat_type = re.sub("[\s+\!\/\[\]\\_,$%^*(+\"\')]+", "", str(Threat_type))
            Threat_type = update_time.replace('\\r', '').replace('\\n', '').replace('\\t', '')
            writer.writerow([chname, cnvd_id, bugtraq_id, release_time, vul_level,
                             impact_producte, cve_id, vul_description1, vul_description2, reference_link,
                             vul_solution, vendor_patch, validation_info, submission_time,
                             included_time, update_time, vul_attachment, Threat_type])
        return true_url

    def sameTargetUrl(self,url):
        same_target_url = []
        for l in self.processUrl(url):
            if re.findall(same_url(url),l):
                same_target_url.append(l)
        for l in same_target_url:
            print(url + '该url页面源码中属于同一域的url有：' + l)
        return same_target_url

    def unrepectUrl(self,url):
        unrepect_url = []
        for l in self.sameTargetUrl(url):
            if l not in unrepect_url:
                unrepect_url.append(l)
        for l in unrepect_url:
            print(url + '该url下不重复的url有------：' + l)
        return unrepect_url

    def crawler(self,crawl_deepth=1):
        while self.current_deepth <= crawl_deepth:
            while not self.linkQuence.unvisitedUrlEmpty():
                visitedUrl = self.linkQuence.popUnvisitedUrl()
                if visitedUrl is None or visitedUrl == '':
                    continue
                links = self.unrepectUrl(visitedUrl)
                self.linkQuence.addVisitedUrl(visitedUrl)
                for link in links:
                    self.linkQuence.addUnvisitedUrl(link)
            self.current_deepth += 1
        print(self.linkQuence.visited)
        return self.linkQuence.visited


def main(i):
    url = 'http://www.cnnvd.org.cn/web/vulnerability/querylist.tag?pageno=' + str(i) + '&repairLd='
    spider = Spider(url)
    spider.crawler(2)

if __name__ == '__main__':
    pool = Pool()
    pool.map(main, (i for i in range(10690,11304)))
